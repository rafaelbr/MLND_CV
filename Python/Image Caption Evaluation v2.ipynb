{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from picturedata.generator import DataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image, sequence\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 383454\n",
      "Vocabulary size: 8256\n",
      "Maximum caption length: 40\n",
      "Image count: 30000\n"
     ]
    }
   ],
   "source": [
    "generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Models/WholeModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 40)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 40, 256)       2113536     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           128128      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 40, 256)       525312      embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 40, 128)       0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 40, 128)       32896       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 40, 256)       0           repeat_vector_1[0][0]            \n",
      "                                                                   time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 1000)          5028000     merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 8256)          8264256     lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 8256)          0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 16,092,128\n",
      "Trainable params: 16,092,128\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog is jumping on a field .\n"
     ]
    }
   ],
   "source": [
    "image_feature = generator.images[1]\n",
    "start = [generator.word_index['<start>']]\n",
    "captions = [[start, 0.0]]\n",
    "while(len(captions[0][0]) < generator.max_cap_len):\n",
    "    temp_captions = []\n",
    "    for caption in captions:\n",
    "        partial_caption = sequence.pad_sequences([caption[0]], maxlen=generator.max_cap_len, padding='post')\n",
    "        next_words_pred = model.predict([np.asarray([image_feature]), np.asarray(partial_caption)])[0]\n",
    "        next_words = np.argsort(next_words_pred)[-3:]\n",
    "        for word in next_words:\n",
    "            new_partial_caption, new_partial_caption_prob = caption[0][:], caption[1]\n",
    "            new_partial_caption.append(word)\n",
    "            new_partial_caption_prob+=next_words_pred[word]\n",
    "            temp_captions.append([new_partial_caption,new_partial_caption_prob])\n",
    "    \n",
    "    captions = temp_captions\n",
    "    captions.sort(key = lambda l:l[1])\n",
    "    captions = captions[-3:]\n",
    "\n",
    "    \n",
    "captions.sort(key = lambda l:l[1])\n",
    "best_caption = captions[-1][0]\n",
    "caption = \" \".join([generator.index_word[index] for index in best_caption])\n",
    "\n",
    "caption_split = caption.split()\n",
    "processed_caption = caption_split[1:]\n",
    "\n",
    "try:\n",
    "    end_index = processed_caption.index('<end>')\n",
    "    processed_caption = processed_caption[:end_index]\n",
    "except:\n",
    "    pass\n",
    "sentence = \" \".join([word for word in processed_caption])\n",
    "print sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_captions = open('Flickr8k/Flickr8k_text/Flickr8k.token.txt', 'rb')\n",
    "captions_text = f_captions.read().strip().split('\\n')\n",
    "image_captions_pair = {}\n",
    "for row in captions_text:\n",
    "    row = row.split(\"\\t\")\n",
    "    row[0] = row[0][:len(row[0])-2]\n",
    "    try:\n",
    "        image_captions_pair[row[0]].append(row[1])\n",
    "    except:\n",
    "        image_captions_pair[row[0]] = [row[1]]\n",
    "    f_captions.close()\n",
    "\n",
    "hypotheses=[]\n",
    "references = []\n",
    "hypothesis = sentence\n",
    "reference = image_captions_pair[generator.data['image'][0]]\n",
    "hypotheses.append(hypothesis)\n",
    "references.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.320570849039\n"
     ]
    }
   ],
   "source": [
    "print nltk.translate.bleu_score.corpus_bleu(references, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
